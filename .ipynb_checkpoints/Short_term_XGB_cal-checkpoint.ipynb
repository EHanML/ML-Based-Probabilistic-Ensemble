{
 "cells": [
  {
   "cell_type": "code",
   "execution_count": 1,
   "metadata": {},
   "outputs": [],
   "source": [
    "# library\n",
    "import pandas as pd\n",
    "import os \n",
    "import feather\n",
    "import numpy as np\n",
    "import matplotlib.pyplot as plt\n",
    "from sklearn.metrics import mean_squared_error\n",
    "import xgboost as xgb\n",
    "from models import xgbm\n",
    "from utils import *"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 2,
   "metadata": {},
   "outputs": [],
   "source": [
    "# read in \n",
    "WU = feather.read_dataframe('Data/WU_lag.feather')\n",
    "\n",
    "#pp\n",
    "WU = WU.drop(columns=['Index','year','ParcelID','Days','year','month'])\n",
    "\n",
    "WU_train = WU[WU['Set'] == 'train'].drop(columns=['Set'])\n",
    "WU_dev = WU[WU['Set'] == 'dev'].drop(columns=['Set'])\n",
    "WU_test = WU[WU['Set'] == 'test'].drop(columns=['Set'])\n",
    "\n",
    "WU_train_all =  WU[WU['Set'] != 'test'].drop(columns=['Set'])\n",
    "\n",
    "predictors = list( WU_train.drop(columns=['TotalWaterUse']).columns)\n",
    "\n",
    "X_train = WU_train.drop(columns=['TotalWaterUse']).values\n",
    "Y_train = WU_train.loc[:,'TotalWaterUse'].values\n",
    "\n",
    "X_dev = WU_dev.drop(columns=['TotalWaterUse']).values\n",
    "Y_dev = WU_dev.loc[:,'TotalWaterUse'].values\n",
    "\n",
    "X_test = WU_test.drop(columns=['TotalWaterUse']).values\n",
    "Y_test = WU_test.loc[:,'TotalWaterUse'].values\n",
    "\n",
    "X_train_all = WU_train_all.drop(columns=['TotalWaterUse']).values\n",
    "Y_train_all = WU_train_all.loc[:,'TotalWaterUse'].values\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 3,
   "metadata": {},
   "outputs": [],
   "source": [
    "\n",
    "from sklearn.metrics import mean_squared_error\n",
    "import lightgbm as lgb\n",
    "import xgboost as xgb\n",
    "from hyperopt import fmin, tpe, space_eval, hp, rand, Trials, STATUS_OK\n",
    "from hyperopt.pyll.stochastic import sample"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "\n",
    "# XGB"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "best_par, ntree = xgbm.xgbBayesVal(X_train, Y_train, X_dev, Y_dev,  max_eval=30)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 5,
   "metadata": {},
   "outputs": [],
   "source": [
    "best_par, ntree  ={'colsample_bytree': 1,\n",
    "  'eta': 0.05,\n",
    "  'gamma': 0.6407480451361361,\n",
    "  'max_depth': 3.0,\n",
    "  'min_child_weight': 84.40381275397019,\n",
    "  'subsample': 0.8},304"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 6,
   "metadata": {},
   "outputs": [],
   "source": [
    "%%capture\n",
    "# dev\n",
    "xgb_model1_dev, ntree, score  = xgbm.XGB_run(X_train, Y_train, X_dev, Y_dev, params = best_par,  verbose = False)\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 7,
   "metadata": {},
   "outputs": [],
   "source": [
    "%%capture\n",
    "# test\n",
    "xgb_model1 , ntree, score  = xgbm.XGB_run(X_train_all, Y_train_all, X_test, Y_test, params = best_par, num_boost_round = ntree, early_stopping_rounds = ntree,verbose = False)\n",
    "\n",
    "xgb_y_hat = xgb_model1.predict(xgb.DMatrix(X_test))"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# XGB calibration"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "best_par, ntree = xgbm.xgbBayesVal2(X_train, Y_train, X_dev, Y_dev, mu_model = xgb_model1_dev, max_eval=20,seed=2)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 8,
   "metadata": {},
   "outputs": [],
   "source": [
    "best_par, ntree = {'colsample_bytree': 0.8,\n",
    "  'eta': 0.05,\n",
    "  'gamma': 0.6854846423792548,\n",
    "  'max_depth': 5.0,\n",
    "  'min_child_weight': 7.838012336950674,\n",
    "  'subsample': 1}, 348"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 9,
   "metadata": {},
   "outputs": [],
   "source": [
    "%%capture\n",
    "# dev\n",
    "xgb_model_dev, ntree, score  = xgbm.XGB_run2(X_train, Y_train, X_dev, Y_dev, mu_model = xgb_model1_dev, params = best_par,  verbose = False)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 10,
   "metadata": {},
   "outputs": [],
   "source": [
    "sig_hat = np.exp(xgb_model_dev.predict(xgb.DMatrix(X_dev)))\n",
    "y_hat = xgb_model1_dev.predict(xgb.DMatrix(X_dev))\n",
    "all_test_MF =  outframe('dev',Y_dev,y_hat,sig_hat)\n",
    "\n",
    "path = 'Out_dev_MF\\\\month1\\\\XGB_dev_1m_dist.feather'\n",
    "feather.write_dataframe(all_test_MF , path)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 11,
   "metadata": {},
   "outputs": [],
   "source": [
    "%%capture\n",
    "# test\n",
    "xgb_model2 , ntree, score  = xgbm.XGB_run2(X_train_all, Y_train_all, X_test, Y_test,mu_model = xgb_model1, params = best_par, num_boost_round = ntree, early_stopping_rounds = ntree, verbose = False)\n",
    "\n",
    "xgb_y_hat = xgb_model1.predict(xgb.DMatrix(X_test))\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 12,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      " & 1132.54 & 8.33 & 5819.65 & 3642.68 & 0.92 & 95\\% \\\\\n"
     ]
    }
   ],
   "source": [
    "sig_hat = np.exp(xgb_model2.predict(xgb.DMatrix(X_test)))\n",
    "y_hat = xgb_model1.predict(xgb.DMatrix(X_test)) \n",
    "zp95 = 1.959963984540\n",
    "left2 = (y_hat - sig_hat*zp95)\n",
    "right = (y_hat + sig_hat*zp95)\n",
    "\n",
    "r1,r2,r3,r4,r5 = get_RMSE_NLL_NOIS_AWPI_ECPI(Y_test,y_hat,left2,right,alpha=0.05)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 14,
   "metadata": {},
   "outputs": [],
   "source": [
    "# results\n",
    "with open(\"Results/Results_1m.txt\", \"a\") as myfile:\n",
    "    myfile.write(\"XGB \\n\")\n",
    "    myfile.write('RMSE %f & NLL %f & NOIS %f & AWPI %f & ECPI %f \\n' % (\n",
    "        r1,r2,r3,r4,r5 ))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 15,
   "metadata": {},
   "outputs": [],
   "source": [
    "all_test_MF = outframe('test',Y_test,y_hat,sig_hat)\n",
    "path = 'Out_test_MF\\\\month1\\\\XGB_1m_dist.feather'\n",
    "feather.write_dataframe(all_test_MF , path)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# BMA"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 16,
   "metadata": {},
   "outputs": [],
   "source": [
    "y_hat_tr = xgb_model1.predict(xgb.DMatrix(X_train_all))\n",
    "sig_hat_tr = np.exp(xgb_model2.predict(xgb.DMatrix(X_train_all)))\n",
    "pt_y_tr = norm.pdf(Y_train_all,loc=y_hat_tr,scale=sig_hat_tr )\n",
    "nll = -np.mean(np.log(pt_y_tr))\n",
    "\n",
    "\n",
    "with open(\"Ensemble/BMA_short.txt\", \"a\") as myfile:\n",
    "    myfile.write(\"XGB, %f \\n\" % (nll))"
   ]
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.8.5"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 2
}
