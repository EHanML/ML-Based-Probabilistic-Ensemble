{
 "cells": [
  {
   "cell_type": "code",
   "execution_count": 1,
   "metadata": {},
   "outputs": [],
   "source": [
    "# library\n",
    "import pandas as pd\n",
    "import os \n",
    "import feather\n",
    "import numpy as np\n",
    "from sklearn.metrics import mean_squared_error\n",
    "import lightgbm as lgb\n",
    "import xgboost as xgb\n",
    "import matplotlib.pyplot as plt\n",
    "from models import lgbm\n",
    "from utils import *\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 2,
   "metadata": {},
   "outputs": [],
   "source": [
    "# read in \n",
    "WU = feather.read_dataframe('Data/WU.feather')\n",
    "\n",
    "#pp\n",
    "WU = WU.drop(columns=['Index','year','ParcelID','Days','year','month'])\n",
    "\n",
    "WU_train = WU[WU['Set'] == 'train'].drop(columns=['Set'])\n",
    "WU_dev = WU[WU['Set'] == 'dev'].drop(columns=['Set'])\n",
    "WU_test = WU[WU['Set'] == 'test'].drop(columns=['Set'])\n",
    "\n",
    "WU_train_all =  WU[WU['Set'] != 'test'].drop(columns=['Set'])\n",
    "\n",
    "predictors = list( WU_train.drop(columns=['TotalWaterUse']).columns)\n",
    "\n",
    "X_train = WU_train.drop(columns=['TotalWaterUse']).values\n",
    "Y_train = WU_train.loc[:,'TotalWaterUse'].values\n",
    "\n",
    "X_dev = WU_dev.drop(columns=['TotalWaterUse']).values\n",
    "Y_dev = WU_dev.loc[:,'TotalWaterUse'].values\n",
    "\n",
    "X_test = WU_test.drop(columns=['TotalWaterUse']).values\n",
    "Y_test = WU_test.loc[:,'TotalWaterUse'].values\n",
    "\n",
    "X_train_all = WU_train_all.drop(columns=['TotalWaterUse']).values\n",
    "Y_train_all = WU_train_all.loc[:,'TotalWaterUse'].values\n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# LGB"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "best_par, ntree = lgbm.lgbBayesVal(X_train, Y_train, X_dev, Y_dev,predictors =predictors,  max_eval=20)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 3,
   "metadata": {},
   "outputs": [],
   "source": [
    "best_par, ntree  = {'bagging_fraction': 0.9727997015067352,\n",
    "  'feature_fraction': 0.5088766499267137,\n",
    "  'max_depth': 20,\n",
    "  'min_data_in_leaf': 64.0,\n",
    "  'num_leaves': 22505.0},92"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 4,
   "metadata": {},
   "outputs": [],
   "source": [
    "%%capture\n",
    "# dev\n",
    "lgb_model1_dev, ntree, score  = lgbm.LGB_run(X_train, Y_train, X_dev, Y_dev, params = best_par, predictors =predictors, verbose = False)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 5,
   "metadata": {},
   "outputs": [],
   "source": [
    "%%capture\n",
    "# test\n",
    "lgb_model1 , ntree, score  = lgbm.LGB_run(X_train_all, Y_train_all, X_test, Y_test, params = best_par, num_boost_round = ntree, early_stopping_rounds = ntree, predictors =predictors, verbose = False)\n",
    "\n",
    "lgb_y_hat = lgb_model1.predict(X_test)\n",
    "\n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# LGB calibration"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "best_par, ntree = lgbm.lgbBayesVal2(X_train, Y_train, X_dev, Y_dev, mu_model = lgb_model1_dev, predictors =predictors,  max_eval=20)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 6,
   "metadata": {},
   "outputs": [],
   "source": [
    "best_par, ntree = {'bagging_fraction': 0.5287061838187463,\n",
    "  'feature_fraction': 0.5016077663494096,\n",
    "  'max_depth': 50,\n",
    "  'min_data_in_leaf': 1348.0,\n",
    "  'num_leaves': 11587.0},204"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 7,
   "metadata": {},
   "outputs": [],
   "source": [
    "%%capture\n",
    "# dev\n",
    "lgb_model_dev, ntree, score  = lgbm.LGB_run2(X_train, Y_train, X_dev, Y_dev, mu_model = lgb_model1_dev, params = best_par, predictors =predictors, verbose = False)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 8,
   "metadata": {},
   "outputs": [],
   "source": [
    "sig_hat = np.exp(lgb_model_dev.predict(X_dev))\n",
    "y_hat = lgb_model1_dev.predict(X_dev)\n",
    "all_test_MF =  outframe('dev',Y_dev,y_hat,sig_hat)\n",
    "\n",
    "path = 'Out_dev_MF\\\\month12\\\\LGB_dev_12m_dist.feather'\n",
    "feather.write_dataframe(all_test_MF , path)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 9,
   "metadata": {},
   "outputs": [],
   "source": [
    "%%capture\n",
    "# test\n",
    "lgb_model2 , ntree, score  = lgbm.LGB_run2(X_train_all, Y_train_all, X_test, Y_test,mu_model = lgb_model1, params = best_par, num_boost_round = ntree, early_stopping_rounds = ntree, predictors =predictors, verbose = False)\n",
    "\n",
    "lgb_y_hat = lgb_model1.predict(X_test)\n",
    "\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 10,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      " & 2505.39 & 9.09 & 11349.61 & 6961.2 & 0.89 & 95\\% \\\\\n"
     ]
    }
   ],
   "source": [
    "sig_hat = np.exp(lgb_model2.predict(X_test))\n",
    "y_hat = lgb_model1.predict(X_test) \n",
    "zp95 = 1.959963984540\n",
    "left2 = (y_hat - sig_hat*zp95)\n",
    "right = (y_hat + sig_hat*zp95)\n",
    "\n",
    "r1,r2,r3,r4,r5 = get_RMSE_NLL_NOIS_AWPI_ECPI(Y_test,y_hat,left2,right,alpha=0.05)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 11,
   "metadata": {},
   "outputs": [],
   "source": [
    "# results\n",
    "with open(\"Results/Results_12m.txt\", \"a\") as myfile:\n",
    "    myfile.write(\"LGB \\n\")\n",
    "    myfile.write('RMSE %f & NLL %f & NOIS %f & AWPI %f & ECPI %f \\n' % (\n",
    "        r1,r2,r3,r4,r5 ))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 12,
   "metadata": {
    "scrolled": true
   },
   "outputs": [],
   "source": [
    "all_test_MF = outframe('test',Y_test,y_hat,sig_hat)\n",
    "path = 'Out_test_MF\\\\month12\\\\LGB_12m_dist.feather'\n",
    "feather.write_dataframe(all_test_MF , path)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# BMA"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 13,
   "metadata": {},
   "outputs": [],
   "source": [
    "y_hat_tr = lgb_model1.predict(X_train_all)\n",
    "sig_hat_tr = np.exp(lgb_model2.predict(X_train_all, num_iteration=ntree))\n",
    "pt_y_tr = norm.pdf(Y_train_all,loc=y_hat_tr,scale=sig_hat_tr )\n",
    "nll = -np.mean(np.log(pt_y_tr))\n",
    "\n",
    "\n",
    "with open(\"Ensemble/BMA_long.txt\", \"a\") as myfile:\n",
    "    myfile.write(\"LGB, %f \\n\" % (nll))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.8.5"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 2
}
